from typing import List
import os

from openai import OpenAI

from domain.interfaces.llm_client import LLMClient
from infrastructure.config import LLM_MODEL, LLM_TEMPERATURE, LLM_MAX_TOKENS, OPENAI_API_KEY


class OpenAIChat(LLMClient):
    """
    OpenAI chat model implementation of the LLMClient interface.
    Uses OpenAI's chat completion API to generate answers based on context.
    """
    
    def __init__(self, model: str = None, temperature: float = None, max_tokens: int = None):
        """
        Initialize the OpenAI chat client.
        
        Args:
            model: The OpenAI model to use (default: from config.LLM_MODEL)
            temperature: Controls randomness in the response (default: from config.LLM_TEMPERATURE)
            max_tokens: Maximum number of tokens in the response (default: from config.LLM_MAX_TOKENS)
        """
        self.client = OpenAI(api_key=OPENAI_API_KEY)
        self.model = model or LLM_MODEL
        self.temperature = temperature if temperature is not None else LLM_TEMPERATURE
        self.max_tokens = max_tokens or LLM_MAX_TOKENS
    
    def answer(self, question: str, context_chunks: List[str]) -> str:
        """
        Generate an answer to a question based on provided context chunks.
        
        Args:
            question: The user's question to answer
            context_chunks: List of text chunks containing context for answering
            
        Returns:
            str: The answer generated by the OpenAI model
        """
        # Format the context chunks into a single string
        formatted_context = "\n\n".join(
            [f"Context chunk {i+1}:\n{chunk}" for i, chunk in enumerate(context_chunks)]
        )
        
        # Create system and user messages
        system_message = {
            "role": "system",
            "content": (
                "You are an intelligent assistant tasked with answering questions "
                "based only on the provided context. "
                "If the answer cannot be found in the context, say 'I don't have enough "
                "information to answer this question.' "
                "Don't use prior knowledge. Be concise and precise."
            )
        }
        
        user_message = {
            "role": "user",
            "content": f"Context information is below:\n\n{formatted_context}\n\nQuestion: {question}"
        }
        
        # Make the API call
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[system_message, user_message],
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        
        # Extract and return the answer
        return response.choices[0].message.content.strip() 