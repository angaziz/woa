# ğŸ§  Smart Assistant Architecture

This project is a modular document-based AI assistant built using Python, following **Onion / Clean Architecture** principles. It ingests documents from disk, chunks and embeds them, stores them in a vector database, and uses an LLM to answer user questions based on semantic search results.

---

## ğŸ—ï¸ Layers Overview

**1. Domain Layer**

- Core models (`Document`, `Chunk`, `Query`, etc.)
- Abstract interfaces (aka ports): `EmbeddingGenerator`, `VectorStore`, `LLMClient`, `DocumentParser`
- No external dependencies

**2. Application Layer**

- Contains business logic and orchestrates use cases
- Example services: `IngestionService`, `QAService`
- Depends on domain interfaces

**3. Infrastructure Layer**

- Implements external integrations:
  - OpenAI for embeddings and LLM
  - Chroma for vector storage
  - Unstructured.io for file parsing
- These classes implement domain interfaces

**4. Interface Layer (UI)**

- Textual CLI interface (for chatting only, in Phase 1)
- Future: REST API or web UI can be added here

---

## ğŸ“¦ Project Structure

```
src/
â”œâ”€â”€ domain/
â”‚ â”œâ”€â”€ models/ # Document, Chunk, Query, etc.
â”‚ â””â”€â”€ ports/ # EmbeddingGenerator, VectorStore, etc.
â”‚
â”œâ”€â”€ application/
â”‚ â”œâ”€â”€ ingestion_service.py # Handles file-to-vector flow
â”‚ â”œâ”€â”€ qa_service.py # Handles question â†’ answer flow
â”‚ â””â”€â”€ tagging_service.py
â”‚
â”œâ”€â”€ infrastructure/
â”‚ â”œâ”€â”€ embedding/ # OpenAI embedder
â”‚ â”œâ”€â”€ vector/ # Chroma vector store
â”‚ â”œâ”€â”€ llm/ # OpenAI chat implementation
â”‚ â”œâ”€â”€ parser/ # Unstructured parser
â”‚ â””â”€â”€ config.py # Loads .env and constants
â”‚
â”œâ”€â”€ ui/
â”‚ â””â”€â”€ cli/ # Textual-based CLI interface
â”‚
â”œâ”€â”€ main.py # Chat runner
â””â”€â”€ ingest_documents.py # Separate entry for ingestion
```

---

## ğŸ“‚ Data Directories

```
data/
â”œâ”€â”€ raw/ # Drop input files here (recursive scan supported)
â”œâ”€â”€ processed/ # Optional: processed metadata, logs, etc.
```

You can simulate document tags via subdirectories (e.g., `data/raw/hr/handbook.pdf` â†’ tag: `hr`).

---

## ğŸ” Swappable Interfaces (Ports)

Located in `domain/ports/`, these allow you to swap implementations without changing business logic.

| Interface          | Description                                 |
| ------------------ | ------------------------------------------- |
| EmbeddingGenerator | Converts text into embeddings               |
| VectorStore        | Stores and retrieves vectorized chunks      |
| LLMClient          | Generates answers using context chunks      |
| DocumentParser     | Extracts clean text from various file types |

---

## ğŸš€ Execution Flow

1. `ingest_documents.py`:
   - Parses and chunks documents
   - Embeds and stores them in Chroma
2. `main.py`:
   - Accepts a user question via CLI
   - Embeds query, retrieves top matches
   - Sends to LLM with context and returns the answer

---

## ğŸ”’ Environment

Set secrets in `.env`:

```env
OPENAI_API_KEY=your-key-here
CHROMA_DB_PATH=chroma/
```
