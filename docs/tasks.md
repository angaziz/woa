# âœ… Smart Assistant â€“ Task Breakdown

---

## ğŸ“ PHASE 1 â€“ Document Ingestion

### Interfaces (Ports)

- [ ] `EmbeddingGenerator` (e.g. OpenAI)
- [ ] `VectorStore` (e.g. Chroma)
- [ ] `DocumentParser` (e.g. Unstructured)

### Infrastructure Implementations

- [ ] `openai_embedder.py`
- [ ] `chroma_store.py`
- [ ] `unstructured_parser.py`

### Domain Models

- [ ] `Document` â€“ filename, content, metadata
- [ ] `Chunk` â€“ chunked content with metadata
- [ ] `Query` â€“ user input + tags

### Application Logic

- [ ] `IngestionService.run(path)` â€“ full ingest pipeline
  - Recursively scan `.data/raw`
  - Parse â†’ Chunk â†’ Embed â†’ Store
  - Assign tags from folder name (e.g. `data/raw/hr/` â†’ tag: hr)

### Entry Point

- [ ] `ingest_documents.py`
  - Wire parser, embedder, vector store
  - Call `IngestionService.run()`

---

## ğŸ“ PHASE 2 â€“ Chat with LLM

> Not yet started â€“ for later milestone

- [ ] `LLMClient` interface
- [ ] `openai_chat.py` implementation
- [ ] `QAService.ask(query)`:
  - Embed query
  - Retrieve chunks
  - Call LLM
- [ ] Textual UI (`ui/cli/main.py`)
- [ ] `main.py` to wire chat

---

## ğŸ› ï¸ Dev Extras

- [x] `.gitignore`
- [x] `.env`
- [x] `requirements.txt`
- [ ] `config.py` to load settings

---

## ğŸ§ª Testing (future)

- [ ] Unit tests for services
- [ ] Integration tests for ingestion
- [ ] Mocking ports for isolation

---

ğŸ“ˆ Roadmap
â€¢ Modular ingestion system
â€¢ LLM-powered QnA system (textualize)
â€¢ Add Confluence, email, or other doc sources
â€¢ Expandable to REST, web UI, other vector DBs or LLMs
